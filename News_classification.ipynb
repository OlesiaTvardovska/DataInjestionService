{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News classification.ipynb",
      "provenance": [],
      "mount_file_id": "1TM1aSVckQ0feYzz-OuBvMxabY9U4SW4H",
      "authorship_tag": "ABX9TyOuxbSFTtVLhOshRHT31I+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlesiaTvardovska/DataInjestionService/blob/dev/News_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdVXduqGTLfs"
      },
      "source": [
        "#News Classification with LSTM using TensorFlow 2.0 and Keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTKyGkl_TTpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e138555b-3d7e-41fe-b537-84b9ba5b3a86"
      },
      "source": [
        "#Libraries import\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import metrics\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK-HRlRXVGKa"
      },
      "source": [
        "# Declaring hyperparameters for the LSTM\n",
        "vocabulary = 5000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oovVal = 'unseenWord'\n",
        "percentOfTrainData = .8"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTwNZ0TKVZQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9798e7eb-cebc-437c-a76a-faa5f69fa992"
      },
      "source": [
        "# Read input data to the DataFrame\n",
        "train=pd.read_csv('drive/My Drive/ValidDataCsv3.csv')\n",
        "train = train[['headline_tok', ' is_arg', ' is_emo_arg_mult',' real_url']]\n",
        "train[train[' is_arg']==''] = 0\n",
        "train[train[' is_emo_arg_mult']==''] = 0\n",
        "train = train.dropna(subset=[' is_arg', ' is_emo_arg_mult'])\n",
        "train.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 54807 entries, 0 to 84590\n",
            "Data columns (total 4 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   headline_tok      54806 non-null  object\n",
            " 1    is_arg           54807 non-null  object\n",
            " 2    is_emo_arg_mult  54807 non-null  object\n",
            " 3    real_url         50339 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 2.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHnoWEWVGRf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d2b5b71a-8c4b-4830-e91b-b129fbab3f91"
      },
      "source": [
        "#Check data has been read in properly\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline_tok</th>\n",
              "      <th>is_arg</th>\n",
              "      <th>is_emo_arg_mult</th>\n",
              "      <th>real_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Школи з мовою навчання національних меншин зак...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>http://hvylya.net/news/digest/shkoli-z-movoyu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Украинская правда  извинилась за распростране...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>http://hvylya.net/news/digest/ukrainskaya-pra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Українських школярів будуть навчати фінансової...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>http://hvylya.net/news/digest/ukrayinskih-shk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Військові пенсіонери 20 січня перекриватимуть ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>http://hvylya.net/news/digest/viyskovi-pensio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Як обстежуватимуть житло претендентів на субсидії</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>http://hvylya.net/news/digest/yak-obstezhuvat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        headline_tok  ...                                           real_url\n",
              "0  Школи з мовою навчання національних меншин зак...  ...   http://hvylya.net/news/digest/shkoli-z-movoyu...\n",
              "1   Украинская правда  извинилась за распростране...  ...   http://hvylya.net/news/digest/ukrainskaya-pra...\n",
              "2  Українських школярів будуть навчати фінансової...  ...   http://hvylya.net/news/digest/ukrayinskih-shk...\n",
              "3  Військові пенсіонери 20 січня перекриватимуть ...  ...   http://hvylya.net/news/digest/viyskovi-pensio...\n",
              "4  Як обстежуватимуть житло претендентів на субсидії  ...   http://hvylya.net/news/digest/yak-obstezhuvat...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Rr98lGtDl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "e2e48490-7996-45a5-afb2-4b28976d94f4"
      },
      "source": [
        "# The fitting column contains only headline\n",
        "train['total']=train['headline_tok']#+train[' is_other']\n",
        "stop = [\"a\",\n",
        "\"б\",\"в\",\"г\",\"е\",\"ж\",\"з\",\"м\",\"т\",\"у\",\"я\",\"є\",\"і\",\"аж\",\"ви\",\n",
        "\"де\",\"до\",\"за\",\"зі\",\"ми\",\"на\",\"не\",\"ну\",\"із\", \"весь\", \"свій\", \"мати\",\n",
        "\"нх\",\"ні\",\"по\",\"та\",\"ти\",\"то\",\"ту\",\"ті\",\"що\",\"як\", \"через\",\n",
        "\"це\",\"цю\",\"ця\",\"ці\",\"чи\",\"ще\",\"що\",\"як\",\"їй\",\"їм\",\n",
        "\"їх\",\"її\",\"або\",\"але\",\"ало\",\"без\",\"був\",\"вам\",\"вас\",\"бути\",\n",
        "\"ваш\",\"вже\",\"все\",\"всю\",\"вся\",\"від\",\"він\",\"два\",\"дві\",\"для\",\n",
        "\"ким\",\"мож\",\"моя\",\"моє\",\"мої\",\"міг\",\"між\",\"мій\",\"над\",\"нам\",\"нас\",\"наш\",\"нею\",\"неї\",\"них\",\"ніж\",\"ній\",\"ось\",\n",
        "\"при\",\"про\",\"під\",\"пір\",\"раз\",\"рік\",\"сам\",\"сих\",\"сім\",\"так\",\"там\", \"який\", \"такий\", \"багато\",\"цей\",\"той\", \"якщо\",\"bbcccnn\", \"ua\"\n",
        "\"будемо\",\"більш\",\"вгору\",\"вміти\",\"внизу\",\"вісім\",\n",
        "\"давно\",\"даром\",\"добре\",\"довго\",\"друго\",\"дякую\",\"життя\",\"зараз\",\"знову\",\"какая\",\"кожен\",\"кожна\",\"кожне\",\"кожні\",\"краще\",\"ледве\",\n",
        "\"майже\",\"менше\",\"могти\",\"можна\",\"назад\",\n",
        "\"немає\",\"нижче\",\"нього\",\"однак\",\"п'ять\",\"перед\",\"поруч\",\"потім\",\"проти\",\"після\",\"років\",\"самим\",\"самих\",\"самій\",\"свого\",\"своєї\",\"своїх\",\"собою\",\"справ\",\"такий\",\"також\",\"тепер\",\"тисяч\",\"тобою\",\"треба\",\n",
        "\"трохи\",\"усюди\",\"усіма\",\"хочеш\",\"цього\",\"цьому\",\"часто\",\"через\",\"шість\",\"якого\",\"іноді\",\"інший\",\"інших\",\"будете\",\"будуть\",\"більше\",\"всього\",\"всьому\",\"далеко\",\"десять\",\"досить\",\"другий\",\"дійсно\",\"завжди\",\"звідси\",\"зовсім\",\n",
        "\"кругом\",\"кілька\",\"людина\",\"можуть\",\"навіть\",\"навіщо\",\"нагорі\",\"небудь\",\"низько\",\"ніколи\",\"нікуди\",\"нічого\",\"обидва\",\"одного\",\"однієї\",\"п'ятий\",\"перший\"]\n",
        "train['total'] = train['total'].astype(str)\n",
        "train['total'] = train['total'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "#!pip install git+https://github.com/kmike/pymorphy2.git@master\n",
        "\n",
        "!pip install pymorphy2-dicts-uk\n",
        "import pymorphy2\n",
        "import pymorphy2_dicts_uk\n",
        "\n",
        "mm = pymorphy2.MorphAnalyzer(lang=\"uk\")\n",
        "train = train[:5000]\n",
        "train['total'] = train['total'].apply(lambda x: ' '.join([mm.parse(word)[0].normal_form for word in x.split() if word not in (stop)]))\n",
        "train['total'] = train['total'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2-dicts-uk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/af/3ac5e9f1d9c31c934beeea852d3af743da561713b03470316f997a5938e1/pymorphy2_dicts_uk-2.4.1.1.1460299261-py2.py3-none-any.whl (5.0MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pymorphy2-dicts-uk\n",
            "Successfully installed pymorphy2-dicts-uk-2.4.1.1.1460299261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-954fdf960a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pymorphy2-dicts-uk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpymorphy2_dicts_uk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2eGrSEmVGaZ"
      },
      "source": [
        "# Create two arrays for texts and labels(true/fake)\n",
        "textsArray = []\n",
        "labels = []\n",
        "\n",
        "textsArray = train['total'].values.tolist()\n",
        "labels = train[' is_emo_arg_mult'].values.tolist()\n",
        "\n",
        "print(len(labels))\n",
        "print(len(textsArray))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWS_lm43SgGm"
      },
      "source": [
        "# Data preprocesssing\n",
        "labels = [item.replace(\" \", \"\") for item in labels]\n",
        "labels = [\"0\" if x == '' else x for x in labels]\n",
        "labels = [float(i) for i in labels] \n",
        "labelsStr = [\"like\" if x < 0.5 else \"dislike\" for x in labels]\n",
        "\n",
        "train[' is_emo_arg_mult']  = labelsStr\n",
        "print(len(labelsStr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99CMTUyxw99N"
      },
      "source": [
        "#Split data into train and test dataset\n",
        "size = int(len(textsArray) * percentOfTrainData)\n",
        "\n",
        "trainData = textsArray[0: size]\n",
        "trainLabels = labelsStr[0: size]\n",
        "\n",
        "testData = textsArray[size:]\n",
        "testLabels = labelsStr[size:]\n",
        "\n",
        "print(len(trainData))\n",
        "print(len(testData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6lAKxGQxMZX"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocabulary, oov_token=oovVal)\n",
        "tokenizer.fit_on_texts(trainData)\n",
        "word_index = tokenizer.word_index\n",
        "dict(list(word_index.items())[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9qGMrrMyZcC"
      },
      "source": [
        "# Turn tokens into lists of sequence\n",
        "print(trainData[3])\n",
        "trainSequences = tokenizer.texts_to_sequences(trainData)\n",
        "print(trainSequences[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2kfpj2zxNcV"
      },
      "source": [
        "# we need sequences in the same size, so we use padding\n",
        "trainList = pad_sequences(trainSequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(len(trainSequences[0]))\n",
        "print(len(trainList[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UmDbT_-xNoC"
      },
      "source": [
        "print(trainList[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa78MWT9xNzL"
      },
      "source": [
        "# Do the same transformations for the test dataset\n",
        "testSequences = tokenizer.texts_to_sequences(testData)\n",
        "testList = pad_sequences(testSequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(len(testSequences))\n",
        "print(testList.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nFWMF46xN-K"
      },
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labelsStr)\n",
        "\n",
        "trainingSeq = np.array(label_tokenizer.texts_to_sequences(trainLabels))\n",
        "testSeq = np.array(label_tokenizer.texts_to_sequences(testLabels))\n",
        "print(trainingSeq[0])\n",
        "print(trainingSeq.shape)\n",
        "\n",
        "print(testSeq[0])\n",
        "print(testSeq.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEHdfHXgxOIY"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_article(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '') for i in text])\n",
        "print(decode_article(trainList[0]))\n",
        "print(trainData[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4UbgiQVxOUT"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
        "    tf.keras.layers.Embedding(vocabulary, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99VsCceEFQvv"
      },
      "source": [
        "!pip install keras-metrics\n",
        "import keras_metrics\n",
        "class_weight = {0: 1.,\n",
        "                1: 50.,\n",
        "                2: 2.}\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "num_epochs = 1\n",
        "history = model.fit(trainList, trainingSeq, epochs=num_epochs,\n",
        "                     validation_data=(testList, testSeq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSPQw6XGluPx"
      },
      "source": [
        "classes=[1,2]\n",
        "y_pred=model.predict_classes(testList)\n",
        "con_mat = tf.math.confusion_matrix(labels=testSeq, predictions=y_pred).numpy()\n",
        "TP = tf.math.count_nonzero(y_pred * testSeq)\n",
        "TN = tf.math.count_nonzero((y_pred - 2) * (testSeq - 2))\n",
        "FP = tf.math.count_nonzero(y_pred * (testSeq - 1))\n",
        "FN = tf.math.count_nonzero((y_pred - 2) * testSeq)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "print(recall)\n",
        "print(precision)\n",
        "print(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbcOyJOQcZ5E"
      },
      "source": [
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * precision * recall / (precision + recall)![alt text](https://)"
      ]
    }
  ]
}